<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Structured Uncertainty Similarity Score (SUSS)</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root {
      --bg: #f8faf6;
      --panel: #ffffff;
      --surface: #fbfbf9;
      --border: #dfe7de;
      --accent: #1f5b3a;
      --accent-soft: #2f7a4f;
      --accent-warm: #f3a76b;
      --muted: #425240;
      --text: #111;
      --shadow: 0 12px 24px rgba(0, 0, 0, 0.08);
      --radius: 10px;
    }

    * {
      box-sizing: border-box;
    }

    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
      margin: 0;
      padding: 0;
      line-height: 1.6;
      background: radial-gradient(circle at 15% 18%, rgba(31, 91, 58, 0.14), transparent 34%),
                  radial-gradient(circle at 72% 12%, rgba(243, 167, 107, 0.12), transparent 34%),
                  radial-gradient(circle at 85% 6%, rgba(47, 122, 79, 0.12), transparent 32%),
                  var(--bg);
      color: var(--text);
    }

    .container {
      max-width: 1180px;
      margin: 0 auto;
      padding: 32px 22px 72px;
    }

    .side-nav {
      position: fixed;
      top: 140px;
      left: 20px;
      width: 200px;
      background: rgba(255, 255, 255, 0.9);
      border: 1px solid var(--border);
      border-radius: 12px;
      box-shadow: var(--shadow);
      padding: 12px;
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .side-nav .nav-title {
      font-weight: 700;
      color: var(--muted);
      padding: 6px 8px;
    }

    .side-nav a {
      text-decoration: none;
      color: var(--text);
      padding: 8px 10px;
      border-radius: 8px;
      font-weight: 600;
      transition: transform 120ms ease, border-color 120ms ease, background 120ms ease;
      border: 1px solid var(--border);
      background: #ffffff;
    }

    .side-nav a:hover {
      transform: translateX(2px);
      border-color: var(--accent);
      background: linear-gradient(120deg, rgba(31, 91, 58, 0.08), rgba(47, 122, 79, 0.08));
    }

    .side-nav a.external {
      border-color: var(--accent-warm);
    }

    .side-nav a.external:hover {
      border-color: var(--accent-warm);
      background: linear-gradient(120deg, rgba(31, 91, 58, 0.08), rgba(243, 167, 107, 0.12));
    }

    .side-nav a.active {
      border-color: var(--accent);
      background: linear-gradient(120deg, rgba(31, 91, 58, 0.12), rgba(47, 122, 79, 0.12));
    }

    @media (max-width: 1100px) {
      .side-nav {
        display: none;
      }
      .container {
        padding: 24px 18px 64px;
      }
    }

    header {
      text-align: center;
      margin-bottom: 28px;
      padding: 28px 22px;
      background: #ffffff;
      border: 2px solid var(--accent-warm);
      border-radius: var(--radius);
      box-shadow: 0 18px 32px rgba(0, 0, 0, 0.08);
    }

    h1 {
      margin: 0 0 10px;
      font-size: clamp(28px, 4vw, 36px);
      letter-spacing: -0.01em;
    }

    .subtitle {
      font-size: 1rem;
      color: var(--muted);
    }

    .authors {
      margin-top: 14px;
      font-size: 0.98rem;
      color: var(--text);
    }

    .authors a {
      text-decoration: none;
      color: var(--accent);
    }

    .authors a:hover {
      text-decoration: underline;
    }

    .button-row {
      margin: 26px 0 8px;
      text-align: center;
      display: flex;
      flex-wrap: wrap;
      gap: 10px;
      justify-content: center;
    }

    .button-row a {
      display: inline-block;
      padding: 18px 26px;
      border-radius: 12px;
      border: 2px solid var(--accent-warm);
      text-decoration: none;
      font-size: 1.12rem;
      font-weight: 700;
      letter-spacing: 0.01em;
      background: #ffffff;
      color: var(--text);
      box-shadow: var(--shadow);
      transition: transform 150ms ease, border-color 150ms ease, background 150ms ease;
    }

    .button-row a:hover {
      transform: translateY(-2px);
      border-color: var(--accent-soft);
      background: linear-gradient(120deg, rgba(31, 91, 58, 0.06), rgba(243, 167, 107, 0.1));
    }

    section {
      margin-bottom: 32px;
      padding: 22px;
      background: var(--surface);
      border-radius: var(--radius);
      border: 1px solid var(--border);
      box-shadow: var(--shadow);
    }

    section h2 {
      margin: 0 0 8px;
      font-size: 1.35rem;
      letter-spacing: -0.01em;
      color: var(--accent);
    }

    .figure {
      margin: 16px 0;
      text-align: center;
      font-size: 0.9rem;
      color: var(--muted);
    }

    .figure img,
    .figure object {
      max-width: 100%;
      height: auto;
      border: 1px solid var(--border);
      border-radius: 10px;
      background: #ffffff;
      box-shadow: var(--shadow);
    }

    .figure-actions {
      margin-top: 8px;
      display: inline-flex;
      gap: 8px;
      flex-wrap: wrap;
      justify-content: center;
    }

    .figure-actions a {
      padding: 8px 12px;
      border-radius: 10px;
      border: 1px solid var(--border);
      text-decoration: none;
      color: var(--text);
      background: var(--surface);
      transition: background 140ms ease, border-color 140ms ease;
    }

    .figure-actions a:hover {
      border-color: var(--accent-warm);
      background: linear-gradient(120deg, rgba(31, 91, 58, 0.06), rgba(243, 167, 107, 0.12));
    }

    .results-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(240px, 1fr));
      gap: 16px;
      margin-top: 16px;
    }

    .result-card {
      border: 1px solid var(--border);
      border-radius: var(--radius);
      padding: 14px;
      background: var(--panel);
      box-shadow: var(--shadow);
      color: var(--text);
    }

    .result-card h3 {
      margin-top: 0;
      font-size: 1.05rem;
      color: var(--accent);
    }

    pre {
      background: #f3f5ef;
      color: #111;
      padding: 16px;
      border-radius: 10px;
      border: 1px solid var(--border);
      overflow-x: auto;
      font-size: 1rem;
      line-height: 1.55;
    }

    pre code {
      font-size: 1rem;
      line-height: 1.55;
    }

    .placeholder-note {
      margin-top: 8px;
      color: var(--muted);
      font-style: italic;
      font-size: 1rem;
    }

    ul {
      margin: 0;
      padding-left: 18px;
      color: var(--muted);
    }

    footer {
      margin-top: 32px;
      font-size: 0.9rem;
      text-align: center;
      color: var(--muted);
    }

    .usage-cards {
      display: grid;
      grid-template-columns: 1fr;
      gap: 16px;
    }

    .results-grid.interpretability-grid {
      grid-template-columns: 1fr;
    }
  </style>
</head>

<body>
  <div class="side-nav">
    <a href="#abstract">Abstract</a>
    <a href="#methods">Methods</a>
    <a href="#usage-examples">Usage</a>
    <a href="#interpretability">Interpretability</a>
    <hr style="width:100%; border:0; border-top:1px solid var(--border); margin:6px 0;">
    <a class="external" href="https://arxiv.org/abs/XXXX.XXXXX" target="_blank" rel="noopener noreferrer">Paper</a>
    <a class="external" href="https://github.com/lopasole02/Structured-Uncertainty-Similarity-Score" target="_blank" rel="noopener noreferrer">Code</a>
    <a class="external" href="checkpoints.html">Checkpoints</a>
  </div>

  <div class="container">

    <header>
      <h1>Structured Uncertainty Similarity Score (SUSS): Learning a Probabilistic,
Interpretable, Perceptual Metric Between Images</h1>

      <div class="authors">
        <a href="https://profiles.sussex.ac.uk/p551890-paula-seidler">Paula Seidler</a>,
        <a href="https://www.ndfcampbell.org/">Neill D F Campbell</a>,
        <a href="https://profiles.sussex.ac.uk/p504012-ivor-simpson">Ivor J A Simpson</a>
      </div>
    </header>

    <!-- buttons -->
    <div class="button-row">
      <a href="https://arxiv.org/pdf/2512.03701" target="_blank" rel="noopener noreferrer">Paper</a>
      <a href="https://github.com/lopasole02/Structured-Uncertainty-Similarity-Score" target="_blank" rel="noopener noreferrer">Code</a>
      <a href="checkpoints.html">Checkpoints</a>
      <a href="#usage-examples">Usage Examples</a>
    </div>

    <div class="figure">
      <img src="figures/teaser_figure_ndfc.png" alt="problem: lack of interpretability in deep-learning based methods" />
    </div>

    <!-- Abstract -->
    <section id="abstract">
      <h2>Abstract</h2>
      <p>
        Perceptual similarity scores that align with human vision are critical for both training and evaluating computer vision models. Deep perceptual losses, such as LPIPS,
        achieve good alignment but rely on complex, highly non-linear discriminative features with unknown invariances, while hand-crafted measures like SSIM are interpretable but miss key perceptual properties.
      </p>
      <p>
        We introduce the <strong>Structured Uncertainty Similarity Score (SUSS)</strong>; it models each image through a set of perceptual components, each represented by a structured multivariate Normal distribution. These are trained in a generative, self-supervised manner to assign high likelihood to human-imperceptible augmentations. The final score is a weighted sum of component log-probabilities with weights learned from human perceptual datasets. 
        Unlike feature-based methods, SUSS learns image-specific linear transformations of residuals in pixel space, enabling transparent inspection through decorrelated residuals and sampling.
      </p>
      <p>
        SUSS aligns closely with human perceptual judgments, shows strong perceptual calibration across diverse distortion types, and provides localized, interpretable explanations of its similarity assessments. 
        We further demonstrate stable optimization behavior and competitive performance when using SUSS as a perceptual loss for downstream imaging tasks.
      </p>
    </section>

    <section id="methods">
      <h2>Methods</h2>
    
      <p>
        SUSS learns a perceptual similarity score by modelling how each image can
        vary under small, human-imperceptible transformations. Instead of relying on
        deep feature embeddings with unknown invariances, we use
        <strong>Structured Uncertainty Prediction Networks (SUPN)</strong> to predict
        multivariate Normal distributions over perceptually close versions of an
        image. This generative formulation allows us to explain the score directly in
        pixel space through residuals, samples, and spatial relevance maps.
      </p>
    
      <div class="figure">
        <img src="figures/fig2_ndfc.png" alt="Overview of the SUSS architecture" />
        <div>
          Our SUPN-UNet predicts the mean and structured covariance of multi-scale luminance
          (Y) and colour (Cb, Cr) components. SUSS evaluates the similarity as a weighted
          sum of component log-likelihoods.
        </div>
      </div>
    
      <p>
        <strong>Perceptual components.</strong>  
        We work in YCbCr and model luminance at three spatial scales, capturing
        fine-to-coarse structure, while chrominance is modelled at a lower
        resolution to reflect reduced colour sensitivity. These components form the
        basis of the final score.
      </p>
    
      <p>
        <strong>Learning perceptual invariances.</strong>  
        SUPN is trained in a self-supervised manner to represent the distribution of
        small geometric and colour transformations that preserve perceptual content.
        These augmentations follow psychophysical findings (e.g., JND-calibrated
        intensity levels). The model predicts a mean image and a
        <strong>sparse Cholesky-factored precision matrix</strong>, enabling efficient
        modelling of correlated spatial and cross-channel structure in pixel space.
      </p>
    
      <p>
        <strong>Similarity through log-likelihoods.</strong>  
        To compare images <em>X</em> and <em>Y</em>, SUSS evaluates how likely each
        component of <em>Y</em> is under the learned distribution around <em>X</em>. The
        component log-probabilities are combined using <strong>learned non-negative
        weights</strong> derived from human perceptual datasets. This yields a
        <em>Mahalanobis-like</em> perceptual distance that is smooth, interpretable, and
        aligned with human judgment.
      </p>
    
      <p style="color: var(--muted);">
        Because the model is fully defined through Normal distributions in pixel
        space, SUSS remains explainable: we can inspect whitened residuals,
        generate samples, and visualise pixel-wise contributions through SUSS maps.
      </p>
    </section>
    

    <!-- Usage Examples -->
    <section id="usage-examples">
      <h2>Try the SUSS Metric</h2>
      <ol style="padding-left: 18px; color: var(--muted);">
        <li><strong>Download a checkpoint + weights</strong>: Pick a SUSS variant (Base, PieAPP, PIPAL, SR) from <a href="checkpoints.html">checkpoints.html</a>. Note the <em>load_path</em> you saved it to and the matching <em>w</em> vector for that variant (replace placeholders).</li>
        <li><strong>Use</strong> (replace <em>load_path</em> and <em>w</em> with chosen variant values):
          <div class="usage-cards">
            <div class="result-card">
              <h3>As a metric (stop grads)</h3>
              <pre><code>from metrics.metric import SUSS

suss_model = SUSS(
    load_path="/path/to/your/suss_variant.pth",
    w=[...],
    testing=True,
    stop_grads=True,
)

score = suss_model.get_log_p_weighted(img_ref, img_dist)</code></pre>
            </div>
            <div class="result-card">
              <h3>As a loss (keep grads)</h3>
              <pre><code>from metrics.metric import SUSS

suss_model = SUSS(
    load_path="/path/to/your/suss_variant.pth",
    w=[...],
    testing=True,
    stop_grads=False,
)

loss = -suss_model.get_log_p_weighted(target, prediction)
loss.backward()</code></pre>
            </div>
          </div>
        </li>
      </ol>
      <p style="color: var(--muted); margin-top: 6px;">For training details and data paths, see the paper and SUPNMetric repo.</p>
    </section>


    <!-- Interpretability / Results -->
    <section id="interpretability">
      <h2>Interpretability Examples</h2>
      <p>
        A key advantage of SUSS is that its similarity score can be interpreted
        directly in pixel space. Because each component is modelled as a multivariate
        Normal distribution, we can visualise how the learned covariance structure
        shapes residuals, highlights perceptually relevant differences, and defines
        the space of variations the model considers perceptually similar. Below are
        three main interpretability tools used in the paper:
      </p>
    
      <div class="results-grid interpretability-grid">
    
        <!-- Whitened residuals -->
        <div class="result-card">
          <h3>Whitened residuals</h3>
          <div class="figure">
            <img src="figures/translation_3_3.png"
                 alt="Whitened residuals example" />
          </div>
          <p>
            SUSS forms the raw residual <em>R = Y − μ(X)</em> and applies the learned
            Cholesky factor <em>L(X)</em> to obtain <strong>whitened residuals</strong>. These show how
            the model decorrelates and rescales differences based on its learned
            perceptual invariances. Irrelevant changes (e.g., small shifts or noise)
            are suppressed, while differences the model has learned to be
            perceptually important become more prominent. The heatmap lets us examine
            regions that contribute strongly to lowering the log-likelihood.
          </p>
        </div>
    
        <!-- SUSS map -->
        <div class="result-card">
          <h3>SUSS map</h3>
          <div class="figure">
            <img src="figures/ze1r_pair.png"
                 alt="SUSS relevance map" />
          </div>
          <p>
            The <strong>SUSS map</strong> combines the squared whitened residuals from all
            luminance scales and colour components into a single spatial relevance
            map using the learned SUSS weights. It provides a pixel-level explanation of the final score: areas with
            high values are where the compared image deviates most from the
            perceptual invariance structure learned around the reference. These
            correspond to the regions that drive the similarity judgement.
          </p>
        </div>
    
        <!-- Samples -->
        <div class="result-card">
          <h3>Samples from the learned distributions</h3>
          <div class="figure">
            <img src="figures/baseno_mean_6.png"
                 alt="Samples from SUPN distributions" />
          </div>
          <p>
            Because SUSS models each component as a multivariate Normal, we can draw
            <strong>samples</strong> to visualise the “perceptual neighbourhood” around an
            image. Close samples preserve fine structural detail; medium and far
            samples show progressively stronger variations, but remain perceptually
            plausible. These samples illustrate the type and strength of changes the
            model assigns high probability to-i.e., which transformations it treats as
            perceptually irrelevant.
          </p>
        </div>
    
      </div>
    </section>    
    
      </div>
    </section>    
    

    <footer>
      SUSS project page &middot; Last updated: 2.12.2025
    </footer>
  </div>
</body>
<script>
  // Highlight current section in side navigation
  const navLinks = Array.from(document.querySelectorAll('.side-nav a')).filter(a => a.hash);
  const sections = navLinks
    .map(link => document.querySelector(link.hash))
    .filter(Boolean);

  const onScroll = () => {
    const scrollPos = window.scrollY + 140; // offset to account for header spacing
    let activeId = null;
    for (const section of sections) {
      if (section.offsetTop <= scrollPos) {
        activeId = section.id;
      }
    }
    navLinks.forEach(link => {
      link.classList.toggle('active', link.hash === `#${activeId}`);
    });
  };

  window.addEventListener('scroll', onScroll, { passive: true });
  onScroll();
</script>
</html>
